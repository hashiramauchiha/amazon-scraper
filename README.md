# ğŸ“¦ Amazon Web Scraping and Analysis Project  

## ğŸ“Œ Overview  
This project is a **Python-based Amazon web scraper** that extracts product details like **title, price, rating, reviews, and links** based on user input. The data is then analyzed to **recommend the best product** using numerical analysis and visualization techniques.  

## ğŸš€ Features  
- âœ… Scrapes product data from Amazon using **BeautifulSoup** and **requests**  
- âœ… Allows users to **input search keywords** and **number of pages** to scrape  
- âœ… Performs **data analysis** on extracted product data (Price, Rating, Reviews)  
- âœ… Uses **Seaborn & Matplotlib** for visualization  
- âœ… Recommends the **best product** based on a calculated score  

## ğŸ“ Project Structure  
```bash
amazon-scraper/
â”‚â”€â”€ amazon_scraper.py       # Web scraper script  
â”‚â”€â”€ form_server.py          # Flask server for user input  
â”‚â”€â”€ templates/  
â”‚   â”œâ”€â”€ index.html          # Web form UI  
â”‚â”€â”€ static/  
â”‚   â”œâ”€â”€ style.css           # Styling for web pages  
â”‚â”€â”€ amazon_products.csv     # Scraped data (Generated)  
â”‚â”€â”€ README.md               # Project Documentation  
â”‚â”€â”€ requirements.txt        # Dependencies  
```

## ğŸ› ï¸ Technologies Used  
- **Python** (BeautifulSoup, requests, pandas)  
- **Flask** (for the web interface)  
- **Matplotlib & Seaborn** (for data visualization)  
- **GitHub** (for version control)  

## ğŸ¯ How It Works  
1. **Run the Flask server** â†’ Opens a webpage to enter a search keyword & number of pages  
2. **Scrapes Amazon** â†’ Extracts product details and saves them in a CSV file  
3. **Performs Data Analysis** â†’ Determines the best product using a score metric  
4. **Displays Results** â†’ Shows analysis graphs and the best product link  

## ğŸ“Œ Installation & Usage  

### ğŸ”¹ Prerequisites  
Ensure you have **Python 3+** and install dependencies:  
```bash
pip install -r requirements.txt
```
## ğŸš€ Running the Project  

### ğŸ”¹ Step 1: Start the Flask Server  
Run the following command in your terminal:  
```bash
python form_server.py
```
### ğŸ”¹ Step 2: Open the Web Interface  
1. Open your browser and go to:  
```bash
http://localhost:5000
```
2. Enter the **search keyword** (e.g., "laptop") and the **number of pages** to scrape. 

![Home page](images/Home.png)

3. Click the **"Scrape"** button to start the process.  
4. The scraper will extract product details and save them in a **CSV file**.  

![Scraped Data](images/Product_Details.png)
5. Once completed, the page will display:  
- âœ… A **link to the scraped data**  
- ğŸ“Š **Visualizations of the analysis**  
- ğŸ† A **link to the best-recommended product**

![Result Page](images/Result.png)

## ğŸ“Š Data Analysis & Visualization  

### ğŸ”¹ Scatter Plot: Rating vs. Reviews  
- **X-axis**: Number of Reviews  
- **Y-axis**: Rating  
- **Bubble Size**: Product Price  
- **Purpose**: Shows the relationship between customer ratings and the number of reviews.  

### ğŸ”¹ Scatter Plot: Price vs. Score  
- **X-axis**: Price  
- **Y-axis**: Score (Calculated as (Rating * Reviews) / Price)  
- **Color Gradient**: Score Intensity  
- **Purpose**: Identifies the most cost-effective product based on rating and popularity.  

### ğŸ† Best Product Selection Criteria  
- The best product is determined using the formula:  
  ```math
  Score = (Rating * Reviews) / Price
  ```
- The product with the highest score is recommended.
### ğŸ“Œ Visualization Output
- The analysis graphs are generated using Matplotlib & Seaborn.
- The graphs are displayed on the results page along with the best product link.
## ğŸ’¡ Future Improvements
- âœ… Implement Scrapy for faster scraping
- âœ… Enhance error handling for blocked requests
- âœ… Deploy on cloud (Heroku/AWS) for remote access
## ğŸ“œ License
This project is for educational purposes only. Amazon does not allow automated scraping, so use it responsibly.
